# 大数据基础

## Hadoop

hadoop内部拆分成几个独立的模块，存储与计算功能独立：

HDFS为分布式文件系统，负责文件存储

MapReduce，MR，计算框架提供计算支持

YARN是资源调度模块，MR程序由yarn来调度运行，同时yarn还可以为其他系统提供调度服务，如spark

### HDFS基本命令

1. 根目录：hdfs dfs -ls,

   hdfs dfs -ls  /

   显示所有：hdfs dfs -ls -R,太多了一般不要直接用，用grep过滤

2. 删除在根目录的空文件夹：hdfs dfs -rm -r \xulintest

   根目录的路径：hdfs://ns15/user/mart_cis/xulin

3. 查看对应目录下的文件：比如查看某个表的文件

   hdfs dfs -ls hdfs://ns15/user/mart_cis/dev.db/dev_op_pop_sku_elasticity_da

4. 文件从HDFS上下载到本地的命令。

   get

   使用方法：hadoop fs -get [-ignorecrc][-crc] <src> <localdst> 

   hadoop fs -get /user/hadoop/file localfile

   hadoop fs -get hdfs://host:port/user/hadoop/file localfile

5. 多级目录-需要一级一级的创建
    首先到hdfs的用户-然后创建
       su hdfs

       hdfs dfs -mkdir /Test
       hdfs dfs -mkdir /Test/first
       hdfs dfs -mkdir /Test/first/data
       hdfs dfs -mkdir /Test/first/data/ok

6. 删除目录首先查看当前用户，再查看准备删除的目录的文件之后--删除，以确保没有误操作
   rf参数表示递归强制删除-尽量不用强制删除
   rm命令删除指定的文件，只删除 非空目录 和 文件
   删除空目录 - r 指示rm将参数中列出的全部目录和子目录均递归地删除
   hdfs  dfsadmin -report
   hdfs dfs -ls  /test/
   hdfs dfs -ls  /test/happy
   hdfs dfs -rm  -r -f /test/happy

7. 报告HDFS的基本统计信息

    hdfs dfsadmin -report

8. 统计多个文件行数

   hdfs dfs -cat  /test/first_data_ok_20129* | wc -l

9. 统计文件大小

   hdfs dfs -count /test/first_data_ok_20129

10. 查看文件的前几行--具体到文件
    hdfs dfs -tail /test/first_data_ok_2012.json

11. 查看部分文件
     hdfs dfs -ls  /Test/tmp/first_data_ok_20151203*



## hive & HBASE

Hbase和Hive在大数据架构中处在不同位置，Hbase主要解决实时数据查询问题，Hive主要解决数据处理和计算问题，一般是配合使用。

区别：

1. Hbase：Hadoop database的简称，也就是基于Hadoop数据库，是一种NoSQL数据库，主要适用于海量明细数据（十亿、百亿）的随机实时查询，如日志明细、交易清单、轨迹行为等；
2. Hive：Hive是Hadoop数据仓库，严格来说，不是数据库，主要是让开发人员能够通过SQL来计算和处理HDFS上的结构化数据，适用于离线的批量数据计算。
   - 通过元数据来描述Hdfs上的结构化文本数据，通俗点来说，就是定义一张表来描述HDFS上的结构化文本，包括各列数据名称，数据类型是什么等，方便我们处理数据，当前很多SQL      ON Hadoop的计算引擎均用的是hive的元数据，如Spark SQL、Impala等；
   - 基于第一点，通过SQL来处理和计算HDFS的数据，Hive会将SQL翻译为Mapreduce来处理数据；

联系：在大数据架构中，Hive和HBase是协作关系，数据流一般如下

1. 通过ETL工具将数据源抽取到HDFS存储；
2. 通过Hive清洗、处理和计算原始数据；
3. HIve清洗处理后的结果，如果是面向海量数据随机查询场景的可存入Hbase；
4. 数据应用从HBase查询数据；

# spark

Spark: spark的核心是一种新型的大数据计算框架，可以基于Hadoop上存储的大数据进行计算（HDFS,hive ）Spark替代Hadoop的一部分，只是替代其计算框架，mapreduce,hive，但spark本身不提供大数据存储;

**spark 和 mr**:

spark的计算模型与MapReduce是完全不同的，spark是基于内存的一种计算框架,有时也会使用磁盘，spark shuffle也会使用磁盘，但是很多操作，比如说简单的map，没有reduce操作，或者是filter类的操作，都是可以直接基于内存进行计算的。

MapReduce的计算模型非常固定，而且死板，必须基于磁盘，以及大量的网络传输;

 所以，spark的速度可以比MapReduce，hive底层也是基于MapReduce来执行SQL语句的，速度快速至少数倍，甚至十倍上百倍。

**spark运行模式**：

spark有很多种模式，最简单就是单机本地模式，还有单机伪分布式模式，复杂的则运行在集群中，目前能很好的运行在 Yarn和 Mesos
中，当然 Spark 还有自带的 Standalone 模式，对于大多数情况 Standalone 模式就足够了，如果企业已经有 Yarn 或者 Mesos
环境，也是很方便部署的。

* local(本地模式)：常用于本地开发测试，本地还分为local单线程和local-cluster多线程;

  * 本地：该模式被称为Local[N]模式，是用单机的多个线程来模拟Spark分布式计算，通常用来验证开发出来的应用程序逻辑上有没有问题。其中N代表可以使用N个线程，每个线程拥有一个core。如果不指定N，则默认是1个线程（该线程有1个core）。

    如果是local[*]，则代表 Run Spark locally with as many worker threads as logical cores on your machine.

    如下： 

    spark-submit 和 spark-submit --master local 效果是一样的

    （同理：spark-shell 和 spark-shell --master local 效果是一样的）

    spark-submit --master local[4] 代表会有4个线程（每个线程一个core）来并发执行应用程序。

  * 本地伪集群：这种运行模式，和Local[N]很像，不同的是，它会在单机启动多个进程来模拟集群下的分布式场景，而不像Local[N]这种多个线程只能在一个进程下委屈求全的共享资源。通常也是用来验证开发出来的应用程序逻辑上有没有问题，或者想使用Spark的计算框架而没有太多资源。

    用法是：提交应用程序时使用local-cluster[x,y,z]参数：x代表要生成的executor数，y和z分别代表每个executor所拥有的core和memory数。

      ```spark-submit --master local-cluster[2, 3, 1024]```

    （同理：spark-shell --master local-cluster[2, 3, 1024]用法也是一样的） 

    上面这条命令代表会使用2个executor进程，每个进程分配3个core和1G的内存，来运行应用程序。 

    SparkSubmit依然充当全能角色，又是Client进程，又是driver程序，还有点资源管理的作用。生成的两个CoarseGrainedExecutorBackend，就是用来并发执行程序的进程。

     运行该模式依然非常简单，只需要把Spark的安装包解压后，改一些常用的配置即可使用。而不用启动Spark的Master、Worker守护进程( 只有集群的standalone方式时，才需要这两个角色)，也不用启动Hadoop的各服务（除非你要用到HDFS），这是和其他模式的区别哦，要记住才能理解。

* standalone(集群模式)：典型的Mater/slave模式，不过也能看出Master是有单点故障的；Spark支持ZooKeeper来实现HA; Spark standalone模式下，集群资源调度由master节点复制。standalone模式只支持简单的资源分配策略，每个人物固定数量的core，各job按顺序依次分配资源，资源不够时排队等待。

  * standalone-client:

    和单机运行的模式不同，这里必须在执行应用程序前，先启动Spark的Master和Worker守护进程。不用启动Hadoop服务，除非你用到了HDFS的内容。

    ```start-master.sh```

    ```start-slave.sh -h hostname url:master```

    图省事，可以在想要做为Master的节点上用start-all.sh一条命令即可，不过这样做，和上面的分开配置有点差别，以后讲到数据本地性如何验证时会说。 

    这种运行模式，可以使用Spark的8080 web ui来观察资源和应用程序的执行情况了。

    言归正传，用如下命令提交应用程序

    spark-submit --master spark://wl1:7077

    或者 spark-submit --master spark://wl1:7077 --deploy-mode client

    代表着会在所有有Worker进程的节点上启动Executor来执行应用程序，此时产生的JVM进程如下：（非master节点，除了没有Master、SparkSubmit，其他进程都一样）

    Master进程做为cluster manager，用来对应用程序申请的资源进行管理；

    SparkSubmit 做为Client端和运行driver程序；

    CoarseGrainedExecutorBackend 用来并发执行应用程序；

    注意，Worker进程生成几个Executor，每个Executor使用几个core，这些都可以在spark-env.sh里面配置，此处不在啰嗦。

  * standalone-cluster:

    使用如下命令执行应用程序（前提是已经启动了spark的Master、Worker守护进程）不用启动Hadoop服务，除非你用到了HDFS的内容。

​		spark-submit --master spark://wl1:6066 --deploy-mode cluster

​		客户端的SparkSubmit进程会在应用程序提交给集群之后就退出;

​		Master会在集群中选择一个Worker进程生成一个子进程DriverWrapper来启动driver程序;

​		而该DriverWrapper 进程会占用Worker进程的一个core，所以同样的资源下配置下​会比第3种运行模式，少用1个core来参与计算(观察下图executor id 7的core数);应用程序的结果，会在执行driver程序的节点的stdout中输出，而不是打印在屏幕上;


* on yarn(集群模式)：运行在 yarn 资源管理器框架之上，由 yarn 负责资源管理，Spark 负责任务调度和计算;

  * on client:

    现在越来越多的场景，都是Spark跑在Hadoop集群中，所以为了做到资源能够均衡调度，会使用YARN来做为Spark的Cluster Manager，来为Spark的应用程序分配资源。

    在执行Spark应用程序前，要启动Hadoop的各种服务。由于已经有了资源管理器，所以不需要启动Spark的Master、Worker守护进程。相关配置的修改，请自行研究。

    使用如下命令执行应用程序

    spark-submit --master yarn 

    或者 spark-submit --master yarn --deploy-mode client

    提交应用程序后，各节点会启动相关的JVM进程，如下：

    在Resource Manager节点上提交应用程序，会生成SparkSubmit进程，该进程会执行driver程序。

  * on cluster:

    使用如下命令执行应用程序:

    spark-submit --master yarn --deploy-mode cluster

    和第5种运行模式，区别如下：

    在Resource Manager端提交应用程序，会生成SparkSubmit进程，该进程只用来做Client端，应用程序提交给集群后，就会删除该进程。

     

    Resource Manager在集群中的某个NodeManager上运行ApplicationMaster，该AM同时会执行driver程序。紧接着，会在各NodeManager上运行CoarseGrainedExecutorBackend来并发执行应用程序。

* on mesos(集群模式)：运行在 mesos 资源管理器框架之上，由 mesos 负责资源管理，Spark 负责任务调度和计算;

* on      cloud(集群模式)：比如 AWS 的 EC2，使用这个模式能很方便的访问 Amazon的 S3;Spark 支持多种分布式存储系统：HDFS 和 S3;



