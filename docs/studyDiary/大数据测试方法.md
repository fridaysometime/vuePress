### 大数据测试方法

说明：总结分为两部分

第一部分：整体的总结

第二部门：详细的说明

**大数据测试思路汇总**

1.  梳理整体业务流程与集市中数据各表的关联关系，从宏观全局的角度对数据产品有一定的认识、了解；
2. 前置阶段通过代码走查方式熟悉各个模块中的业务及详细的逻辑，确定测试重点，以便后续对重点逻辑采用MOCK数据、单元测试等多种方式验证；
3. 测试阶段通过样本数据、单元测试、小模块切分、全量数据及以测试角度开发数据加工脚本并进行与研发脚本数据结果对比等方式进行多样化验证；
4. 监控阶段通过数据测试平台的监控模块对集市表的数据枚举值异常监控报警方式进行，保障线上数据产品的稳定；

**模型测试思路汇总（模型篇）**

**模型测试考虑如下几步（适用于分类、回归等有监督学习问题，聚类等非监督学习问题暂未有好的思路）：**

针对模型测试需要考虑几个点：

1） 就是需要评估特征选取的依据，全面性和特征权重

2） 主要特征值枚举的转化，即是否把重要特征的枚举值转化为了相应的数值向量(一维)，有遗漏的话则对某些数据的预测有偏差

3） 标准验证数据集的选取

4） 模型预测数据与验证数据真实数据标准差的阈值的定义

**下面是解释：**

**l** **特征选取**

如何进行特征选取，目前的想法是在产品需求的阶段和产品、研发进行讨论，了解选取哪些特征，如何选的？

后面考虑如何进行特征选取的评估 --跟汪雷等进行讨论

**l** **特征值数值向量转化**

特征值，文字转化为数值，将特征值枚举进行数值转化的过程中某些值未转化等

**l** **数据集的选取及阈值定义**

通过对全数据集（还未划分训练、验证）采用数据分析方法，分析数据多样性，各类数据占比，按数据分布特点抽样出测试集（这块功能需要进行调研思考），通过模型服务调用方式获得预测值，并对预测值进行均方误差和准确率等类似方法进行分析，看是否达到与产品确认的目标【准确比】占比，若未达到，则通知开发继续优化模型，再次多次验证，直到达到目标

# **数据模型类+业务类的项目测试：**

**测试步骤如下：**

# **一.需求阶段:**

1. 参与需求评审，了解项目需求及使用场景，熟悉数据指标要求、数据服务对象、业务逻辑等；

2. 了解数据与业务的映射关系，方便后续的测试工作开展；

# **二.测试前置开展：**

研发阶段通过白盒测试开展测试前置

## **1、代码走读**

通过代码走读，了解每个模块的作用，输入输出，及整体数据流转过程，验证数据源使用是否合理、是否满足需求，加工逻辑是否完善、是否符合预期，有利于提早发现隐藏问题，降低项目风险

在代码较长，前后逻辑关系紧密的情况下，需要梳理整体流程，便于宏观掌握思想：

A. 以功能模块为单位，在excel上梳理出源表表名，源表分区字段，源表重要字段备注说明，中间表表名，中间表分区字段，结果表表名，结果表分区字段，源表为中间表提供的字段+概述加工逻辑（比如中间表A字段是按照源表的xxx分组后对yyy累加后取超过zzz的值），中间表为结果表提供的字段，页面与结果表数据的映射关系。

其中，源表重要字段的调研工作可为日后测试奠定业务基础，拓展测试层面，尽可能避免业务点遗漏，调研涉及：

i. 记录源表中划分数据业务类型：比如sku上下柜状态，订单有无效状态，渠道,；

ii. 记录源表划分统计范围：比如销量为空或为0和正常销量；gmv为负数和正常gmv；

iii. 记录源表中相似字段：比如main_brand_id,brand_id；

iv. 记录源表中过滤条件，分组字段，需要验证过滤条件是否符合业务需求，分组字段是否完备，且通过分组后的数据满足后续的处理逻辑；

B. 针对pyspark代码，以方法为单位，在excel中罗列出每一个方法中的dataframe的列，备注上重要列的数据特点，比如存在重复值，存在空值等等可能对日后dataframe之间连接有影响的特点。其中，如果dataframe是由多个子dataframe连接而来，则备注上大致的连接逻辑和业务说明。

C. 针对scala代码，在excel中罗列出每一个rdd的列，备注上重要列的数据特点，记录针对该rdd的map操作，并记录由此产生的新列数据。

## **2、单元测试**

对重点模块做单元测试，验证重点的单元逻辑处理；

对很多不易创造测试场景的复杂模块做单元测试，简化后续测试工作

例如：在代码走读，梳理逻辑时发现大量的数学运算都是针对某个中间表或者dataframe或者rdd的基础上进行的，那么这个中间表或者dataframe或者rdd的准确性就尤为重要，针对生成这个至关重要节点的方法需要进行单元测试。

A. 设置多组入参，验证正向逻辑的计算逻辑，入参设置为正常数据；验证代码能否处理异常逻辑，设置入参为一些边界值，空值，异常值等等（但有可能**入参中可能会出现一些根本不存在的场景**，针对这种场景，首先从入参的来源上进行分析，第一要调研业务数据背景，第二要排查生成该入参的过程，查询相关表中取值，判断是否有可能存在这种入参；最后要和开发确认后才可排除）

# **三、数据加工测试**

## **1、测试数据准备**

l 测试数据不在数量的多少，而在于覆盖的全面性

l 根据需求及前期了解的业务逻辑和加工逻辑，伪造测试数据及对应的期望结果（并包含特殊逻辑，特殊数据，特殊值）测试数据可以是一个表，一个字段，或者一个数据集

l 对于复杂的算法，需要进行逻辑拆分，分成几个部分，通过中间结果验证

测试数据举例：

A. 使用pyspark,从数据加工使用的数据源表中按照一定维度（比如某个三级品类）,生成一个dataframe，作为入参

B. 编造数据，使用pyspark生成一个dataframe，作为入参

C. 人工根据入参所需要的列，将编造的数值写入txt，从文本读入

D. 测试数据取源表中的多个维度但数量级减少，完备且多样

## **2、运行被测代码**

A. 本地安装pyspark或者scala环境，处理期望执行文件

i. 处理输入端：替换文件中读入的集市源表，改为本地txt或者在代码中写测试数据；增加main函数启动入口；

ii. 处理中间过程展示：在代码中间增加打印语句，展示想关注的中间某个结果

iii. 处理输出端：注释insert overwrite语句，改为打印，方便调试和看结果

B. 堡垒机运行，处理期望执行文件；事先理清被测代码的运行逻辑，特别关注于代码读取多个配置文件的逻辑；以及启动脚本可能是shell脚本，此时有可能需要修改启动脚本中的参数以及配置文件中的库表名。

i. 使用spark submit方式：

关于源表修改——两种方案，使用代码自身使用的源表或者将自己本地的测试数据txt利用load命令写入test层表，事先在test层建表，表结构为代码中的源表表结构；

关于目标表修改:在提交前需要将代码中涉及insert overwrite的表统一换成test层表，事先在test层建表，表结构为代码中的目标表结构；

将代码处理逻辑分模块，在每一个重点逻辑后，将结果打印到log文件里或者临时文件里，通过对log文件少量数据的展示或临时文件中大量数据的分析，验证中间的处理逻辑是否正确；

ii. 使用pyspark或者scala的交互式界面：

关于源表和目标表的处理如上spark submit方式。将开发代码逐句或者多句粘贴到堡垒机执行，跟踪每一步结果，对每一个步的结果做业务和数据上的分析，保证其输出的数据对下一步数据处理是正确的入口。

## **3、期望结果与实际结果比对**

运行测试代码，通过表形式比对，也可以通过文件形式比对

## **4、模拟线上运行（即全量数据）**

全量运行线上数据，查看运行结果

**l** **数据海量复杂性**

对线上数据做汇总验证，对计算过程出涉及的维度、计算前后的数据条数比对，转换方法计算结果比对等的汇总验证，验证数据正确性：如， join前数据来源与join后数据结果，某个维度计算前条数，计算后总数等； 

**l** **数据多样性**

由于大数据的数据多样性，总会出现计算结果的效果不如人意，需要根据多样性数据做特殊处理，以确保最终结果展示满足需求

使用抽样测试，抽取一些一般认知了解的数据集，进行抽样测试，或从前端报表形式，发现特殊数据共性，提出问题加以处理

## **5、数据加工中insert-overwite测试**

针对在原有hive表中增加列的需求，需要使用hive-sql检查增加的列是否为空，若表结构正确，增加列数据却为空，应进一步与数据加工开发核实其insert overwrite的操作步骤。

注：增加列，应先drop已存在的所有分区老数据，再insert overwite全量数据。

# **四、模型测试**

对有已知结果的监督学习训练的模型，可重点分为两步进行测试：对训练集数据的测试以及对模型在测试集上的效果测试；

## **1、训练集数据的测试：**

与大数据测试类似，以调底层开源算法包之前的全量数据为输入，正向验证数据源处理的代码层的规则是否满足业务需求，一步步的分析结果；

A. 数据源测试

i. 针对离线训练的模型，拿到同开发一致的数据源文件；编写测试脚本对数据源数据的多维数据进行统计分析；可验证单维度的字段数据（例如：极值是否符为异常；所有枚举中是否有异常等） ；

ii. 可验证多维度组合数据（例如：包括组合维度数据是否有异常）；

iii. 数据加工的数据若存在task分区，往往表明加工的数据存在多版本，此时应检查模型使用的task版本是否为满足业务需求的版本

B. 数据预处理测试：

走读预处理代码，针对数据源测试的结果进行以下处理：

i. 单个维度的所有枚举值是否在后续处理逻辑中都覆盖；

ii. 单个维度的异常值是否有符合业务逻辑的处理；

iii. 单个维度的空值，缺失值是否有符合业务逻辑的处理；

iv. 排查数据的处理逻辑是否正确，是否在join或者组合新list的过程中拿到的新数据并非满足业务需求；

C. 特征重要性反推特征数据

i. 训练模型代码修改，可得到特征属性的重要性；对重要性偏低的特征反查该特征数据是否正确

ii. 训练模型代码修改，可得到特征属性的重要性；对重要性偏低的特征提出建议性意见，比如移除该特征或特征融合；

## **2、** **模型验证：**

测试集数据，验证训练模型的准确性；

A. 开发一般会预留数据源中的一部分数据作为测试集；我们也将这部分数据作为测试集使用，编写测试脚本，设置阈值（即误差范围的阈值），多次调用已经生成的模型，不计算平均指标，而计算单条记录的预测值和真实值的均方差（或其他评估指标）；计算不同的阈值范围，超过该阈值的记录数占整个测试集的记录数的占比；如果超过误差范围的占比太多，说明模型不准确，需要重新选择特征或处理数据。

B. 以真实数据为导向，验证模型

使用历史的已有真实数据，与模型运行出来的结果做对比，查看模型与真实数据的差距，来验证模型的效果

C. 利用集市现有数据进行测试，使用大数据平台数据订阅功能，订阅所需数据（包含所需入参和预期结果）获取csv格式文件，利用python-pandas将csv中的所需列读入，遍历该列作为入参，调用模型方法，将模型返回结果存入list中,和csv中的预期结果列list进行比较；

D. 利用网络爬虫获取真实测试数据，使用scrapy框架将爬取结果存入csv(包含所需入参和预期结果)，使用该csv方法同集市订阅csv数据一致。

## **3、业务规则验证：**

在模型调用前后，需要根据业务需求对数据处理，得到模型的入参；对模型出参需要一定的业务规则输出，这些业务规则与模型紧密相关，并影响模型结果。

A. 单元测试

通过单元测试，对代码逻辑测试。根据业务需求制定的规则，对代码做各种业务场景测试，包含异常值、特殊值等 

B. 数据标签

对于数据量较大的入参，可以通过在代码中添加中间结果文件，对每个结果使用不同标记，验证输出结果是否正确，如京东趋势测试

C. 黑盒测试

在明确业务规则的输出结果前提下，将模型代码块看做黑盒，设计入参，检查出参。例如FP树频繁项集算法代码，检查出参的频繁项统计。该频繁项往往用于重要程度的指标计算，结合业务判断重要程度是否过于集中，区分度小，不利于业务层面的后续使用。

# **五、数据展示系统测试（基本同数据可视化的用户层测试）**

大数据处理后的数据会同步到mongo或mysql，方便前端展示，前端通过http方式向后台获取数据，并对数据处理展示

**A.** **数据获取**

将数据获取转化为http接口测试，做成接口自动化，重复使用，提供工作效率

**B.** **前端展示**

通过接口mock，模拟需要测试的各种场景，以达到对数据多样性的覆盖全面性

**C.** **其他**

检查数据加工的目标表中的空值,在导入mysql或者mongo或者ES时，是否已在导入过程中被处理，或者在java代码中处理，避免系统界面涉及的查询及图表展示异常。

 

大数据相关项目测试流程

1、 需求了解

业务需求

数据计算逻辑及数据源

2、 测试设计

测试用例编写

测试数据准备（测试集市数据同步及构造测试数据）

3、 测试阶段

数据加工测试：源码单元测试（验证计算逻辑，数据源）,测试集市任务执行（全覆盖）,运行结果验证

模型测试：根据模型特点进行测试设计及数据准备

业务系统测试：常规系统测试 ,导入测试加工数据至业务表,验证数据层和业务层是否匹配,根据业务层展示优化数据效果

4、 线上验证

验证加工数据是否完整

验证业务系统对线上数据展示

 