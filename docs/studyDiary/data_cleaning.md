---
title: data_cleaning
date: 2019-03-04
tags: 
- 机器学习
- 统计学
---
# data_cleaning

## 缺失值

1. 直接使用：模型支持缺失值，决策树
2. 删除特征： 某特征大多数是缺失值，可删除特征
3. 补全： 
* 均值填充： 值都一样
* 众数
* 插值法
* 聚类，同类均值补充
* 建模预测：如果缺失属性与其他属性无关，那么预测结果无意义。如果高度相关，那么可以删除特征
* 高维映射：最准确的做法，因为完全保留了信息，也不增加任何信息；样本量大才可以

实例
```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
pd.set_option('display.max_columns',10)
raw=pd.read_csv('winequality-red.csv')
```

## 异常值
指一组测定值中与平均值的偏差超过两倍标准差的测定值。与平均值的偏差超过三倍标准差的测定值，称为高度异常的异常值
1. 确认方法：画box图检查；用3倍std检查

```python 
plt.boxplot(raw.chlorides)
plt.show()
```

2. 解决方法：
* 空值：视为空值待后期对空值进行处理，填补；但填补方式的不同会不同程度的改变原有的数据分布；
* 盖帽法：重新设定数据边界，也会改变数据的分布，但异常值往往很少的话可以采用；整行替换数据框里99%以上和1%以下的点，将99%以上的点值=99%的点值；小于1%的点值=1%的点值。即
把3sigma之外的数据定为sigma

```python
mu=raw.chlorides.mean()
sigma=raw.chlorides.std()
lb=mu-3*sigma
hb=mu+3*sigma
tmp[tmp<lb]=lb
tmp[tmp>hb]=hb
plt.boxplot(raw.chlorides)
plt.show()
```

* 取对数：
```python
raw.chlorides.hist(figsize=(8,4),bins=40)
plt.show()
raw['chlorides_expd']=np.log(tmp+1)
raw.chlorides_expd.hist(figsize=(8,4),bins=40)
plt.show()
```


* 分类建模：把干扰变量变成分类变量（异常为1，不异常为0）
* 离散化：例如做成 高、中、低，三种字段。
* 剔除 ：整行剔除，整列剔除
* 变量转换：通过一定的变换改变原有数据的分布，使得异常值不在异常；（对比上面的取对数）
对严重右偏的数据非常有用，变换后的数据能够更接近正态分布；
![https://pic3.zhimg.com/80/v2-f8b7c166c351fd3a036b69a888ec1132_hd.jpg][标记1]

## 冗余值
* 删除冗余值：drop_duplicates()

## 模型反馈
1. 数据清洗有没有问题
2. 数据抽样有没有问题
3. 数据理解有没有问题：主成分分析，聚类看一下
4. 模型选择有没有问题
5. 参数调整有没有问题

[标记1][https://pic3.zhimg.com/80/v2-f8b7c166c351fd3a036b69a888ec1132_hd.jpg]

## 分类变量

提供了重要的信息，但可能表达方式是纯文字或者与文字相关的整数，那么建模时往往需要将分类变量量化，仅用简单的id和原始形式不行；

1. sklearn- preprocessing.OneHotEncoder()

   原理：将离散型特征的每一种取值[都看](https://www.baidu.com/s?wd=%E9%83%BD%E7%9C%8B&tn=24004469_oem_dg&rsv_dl=gh_pl_sl_csd)成一种状态，若你的这一特征中有N个不相同的取值，那么我们就可以将该特征抽象成N种不同的状态，one-hot编码保证了每一个取值只会使得一种状态处于“激活态”，也就是说这N种状态中只有一个状态位值为1，其他状态位都是0。

   编码器为每个分类变量创建了额外的特征，转变成一个稀疏矩阵。矩阵是这样定义的：每一行由0和1构成，对应的分类特征是1，其他都是0。用稀疏矩阵存储数据很合理。

2. sklearn-DictVectorizer

   可以将字符串转换成分类特征

3. dummy variable

   哑变量编码直观的解释就是任意的将一个状态位去除；哑变量编码觉得one-hot编码太罗嗦了（一些很明显的事实还说的这么清楚），所以它就很那么很明显的东西省去了。

   比如一个分类变量有3个值，用OneHotEncoder()表示：

   100

   010

   001

   而用哑变量表示的话，2个状态就可以表示2个值：

   10

   01

   00

## 连续变量

1. bining-数据分箱

   数据分箱：按照某种规则将数据进行分类；相当于数值连续变量数量太多且真实的数据的大小对模型的影响没有实际价值，不如将变量分类，跟学生成绩排ABC一样，100-91的成绩为A，70-89的为B；然后按照处理分类变量一样处理；

2. regularization-正则化

   特征太多可能会产生过拟合，为防止过拟合，采用正则化方法减小不重要的特征变量，自动提取重要的变量，减小特征变量的数量级；

## 数据类别不均

1. 重新采样
   * 欠采样：
     * 随机地消除占多数的类的样本来平衡类分布；直到多数类和少数类的实例实现平衡，目标才算达成。
     * 缺点：丢弃有价值的重要的信息；欠采样选取的样本可能有偏差，不能代表大多数；
   * 过采样
     * 随机复制少数类来增加其中的实例数量，从而可增加样本中少数类的代表性。
     * 缺点：由于复制少数类事件，它加大了过拟合的可能性。
 2. 算法集成技术

    * bagging: 
      * 集成方法的主要目的是提高单个分类器的性能。该方法从原始数据中构建几个两级分类器，然后整合它们的预测。
      * 缺点：bagging 只会在基本分类器效果很好时才有效。错误的分类可能会进一步降低表现。

    * boosting:
      * 它可以将弱学习器结合起来创造出一个能够进行准确预测的强大学习器。Boosting 开始于在训练数据上准备的基本分类器/弱分类器
    * 例如：imblearn.ensemble中的EasyEnsemble做集成处理
3. 扩充数据集：首先想到能否获得更多数据，尤其是小类（该类样本数据极少）的数据，更多的数据往往能得到更多的分布信息；
4. 人造数据：
   * 一种简单的产生人造数据的方法是：在该类下所有样本的每个属性特征的取值空间中随机选取一个组成新的样本，即属性值随机采样。此方法多用于小类中的样本，不过它可能破坏原属性的线性关系。如在图像中，对一幅图像进行扭曲得到另一幅图像，即改变了原图像的某些特征值，但是该方法可能会产生现实中不存在的样本。
5. 通过正负样本的惩罚权重解决样本的不均匀
   * 对于分类中不同样本数量的类别分别赋予不同的权重，一般是小样本量类别权重高，大样本量类别权重低。sklearn中针对一些模型会有相应的操作，例如svm, class_weight='balanced'



